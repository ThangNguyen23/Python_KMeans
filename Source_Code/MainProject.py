# -*- coding: utf-8 -*-
"""MainProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tBzRI4kfcUBOYnUcd9fSwyJhvNOkwkwl
"""

# install library
!pip install openpyxl
# !pip install pqkmeans

# faiss library (CPU & GPU)
# !apt install libomp-dev
# !pip install faiss-cpu
# !python -m pip install --upgrade faiss faiss-gpu

# import library
# import faiss
# import pqkmeans
import time
import numpy as np
import pandas as pd
import datetime as dt
import warnings
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import plotly.express as px
import seaborn as sns
import plotly.graph_objs as go
import sklearn.preprocessing as pp

from plotly.offline import init_notebook_mode, iplot
from sklearn.metrics.pairwise import pairwise_distances_argmin
from sklearn.datasets import make_blobs
from sklearn.preprocessing import MinMaxScaler,StandardScaler, OneHotEncoder
from sklearn.cluster import KMeans, MiniBatchKMeans
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
from IPython.core.pylabtools import figsize
from mpl_toolkits.mplot3d import Axes3D
from IPython.display import display
from ipywidgets import widgets, HBox
from sklearn.metrics import silhouette_score
from sklearn.manifold import MDS
from plotly.subplots import make_subplots
from mlxtend.plotting import plot_learning_curves
from mlxtend.data import mnist_data
from mlxtend.preprocessing import shuffle_arrays_unison

# prevent colab delete data.xlsx when logout
# tutorial video (https://video.twimg.com/tweet_video/EQbtltjVAAA2qTs.mp4)
from google.colab import drive
drive.mount('/content/drive')

# check path xlsx files
import os
import numpy as np
import pandas as pd
for dirname, _, filenames in os.walk('/content/drive/MyDrive'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
# result : /content/drive/MyDrive/online_retail_II.xlsx

# set options
pd.set_option('display.max_columns', None)
pd.set_option('display.float_format', lambda x: '%.5f' % x)

# set warnings
warnings.filterwarnings('ignore')
warnings.simplefilter(action='ignore', category=FutureWarning)

# load data from xlsx files
df_ = pd.read_excel('/content/drive/MyDrive/online_retail_II.xlsx', sheet_name='Year 2009-2010', engine='openpyxl')
df = df_.copy()
df.head()

# exploratory data
def check_df(dataframe):
  print('===================================== Shape =====================================')
  print(dataframe.shape)
  print('===================================== Types =====================================')
  print(dataframe.dtypes)
  print('===================================== Head =====================================')
  print(dataframe.head(5))
  print('===================================== Tail =====================================')
  print(dataframe.tail(5))
  print('===================================== NA =====================================')
  print(dataframe.isnull().sum())
  print('===================================== Quantiles =====================================')
  print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

# run function check_df
check_df(df)

# result :
# + Customer ID is duplicated
# + Customer ID and Description have null values
# + Customer ID will be used for identifying customers on customer segmentation, so null values is both variables will be dropped
# + Price and Quantity variables have values below zero, which is not possible
# + Prices range from 0 to 25111

# begin cleaning data
# check null or empty data
df.isnull().sum()

# drop all null values
df.dropna(inplace=True)
df = df.dropna(subset=['Customer ID'])

# check again null data. we won't be using description information for segmentation so we'll leave them as is
df.isnull().sum().sum()

# check duplicate data
df.duplicated().sum()

# drop all dupticate data
df = df.drop_duplicates()

# check again duplicate data for sure it is removed
df.duplicated().sum()

# overall about of data
df.describe()

# check 1 randomly picked customer's shopping history
df.loc[df['Customer ID'] == 13085.00000]

# when a customer makes a purchase, the Invoice is duplicated and each unique item that has been purchased are transformed into seperate observations
# check number of unique customers
df['Customer ID'].nunique()

# which countries are the customers from
countries = df['Country'].value_counts()
countries

# simple map of countries
data_map = dict(type='choropleth', locations = countries.index, locationmode = 'country names', z = countries, text = countries.index, colorbar = {'title':'Row nb.'}, colorscale=[[0, 'rgb(224,255,255)'], [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'], [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'], [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'], [1, 'rgb(227,26,28)']], reversescale = False)
layout = dict(autosize=False, width=1000, height=600,title='Number of rows per country', geo=dict(showframe=True, projection={'type':'mercator'}))

map_of_countries = go.Figure(data=[data_map],layout=layout)
iplot(map_of_countries, validate=False)

# percentage customers from UK and Outside UK
x = df['Country'].apply(lambda x: x if x == 'United Kingdom' else 'Not United Kingdom').value_counts().rename('Total Rows')
y = (x/df.shape[0]).rename('% Rows')
pd.concat([x, y], axis= 1)

# customers without country information
df.loc[df['Country'] == 'Unspecified', 'Customer ID'].value_counts()

# create array for 5 customers missing country information
missing_country = [16320.00000, 14265.00000, 12470.00000, 12351.00000, 15357.00000]
df.loc[df['Customer ID'].isin(missing_country)].head(5)

# we won't be using Country information for segmentation so we'll leave them as is

# do each StockCode belong to a specific Item because 1 StockCode have many Descriptions
df.groupby('StockCode').agg({'Description' : 'nunique'}).sort_values(by='Description', ascending=False).head()

# check all descriptions of 1 randomly StockCode
df.loc[df['StockCode'] == 22384, 'Description'].unique()

# 4 item names above refer to the same item and StockCode. This duplicate may be due to manual entry. StockCode is a better variable to analyze items because it's unique

# check the observations that have negative value for Quantity
df.loc[df['Quantity'] < 0].head(10)

# check the Invoices that starts with the letter "C"
df.loc[df['Invoice'].str.startswith('C', na=False), ['Quantity', 'Price']].describe()

# invoices that start with "C" are refunds, the Quantity values are below 0

# remove refund invoices
df = df[~df['Invoice'].str.contains('C', na=False)]
df.describe().T

# some transaction have a sale price of zero, these could be free gifts for some customers and not true sales transaction. So, we will drop them.
indx = df.loc[df['Price'] == 0].index
df.drop(index= indx, inplace= True)

# create heatmap for cohort analysis
df_heat = df.copy()
print(df_heat.shape)

# for cohort analysis, there are a few labels that we have to create :
# + invoice period : a string representation of the year and month of a single transaction/invoice.
# + cohort group : a string representation of the the year and month of a customer’s first purchase. This label is common across all invoices for a particular customer.
# + cohort period / cohort index : a integer representation a customer’s stage in its “lifetime”. The number represents the number of months passed since the first purchase.

# data for heatmap
# data cointains from 2009/12/01 to 2010/12/09
def get_month(x):
  return dt.datetime(x.year, x.month, 1)

df_heat['InvoiceMonth'] = df_heat['InvoiceDate'].apply(get_month)
grouping = df_heat.groupby('Customer ID')['InvoiceMonth']
df_heat['CohortMonth'] = grouping.transform('min')
df_heat.tail()

# get year, month, day and calculate cohort index
def get_month_int(dframe, column):
  years = dframe[column].dt.year
  months = dframe[column].dt.month
  days = dframe[column].dt.day
  return years, months, days

invoice_year, invoice_month,_ = get_month_int(df_heat, 'InvoiceMonth')
cohort_year, cohort_month,_ = get_month_int(df_heat, 'CohortMonth')

year_diff = invoice_year - cohort_year
month_diff = invoice_month - cohort_month

df_heat['CohortIndex'] = year_diff * 12 + month_diff + 1

# count monthly active customers from each cohort
grouping = df_heat.groupby(['CohortMonth', 'CohortIndex'])
cohort_data = grouping['Customer ID'].apply(pd.Series.nunique)

# return number of unique elements in the object
cohort_data = cohort_data.reset_index()
cohort_counts = cohort_data.pivot(index= 'CohortMonth', columns= 'CohortIndex', values= 'Customer ID')
print(cohort_counts)

# customer retention is a very useful metric to understand how many of the all customers are still active.
# retention gives you the percentage of active customers compared to the total number of customers.

# create retention rate table
# get column 0 as a SERIES of shape (n,) (cohort sizes)
cohort_size = cohort_counts.iloc[:,0]
# axis=0 to ensure the divide along the row axis
retention = cohort_counts.divide(cohort_size, axis= 0)
# to show the number as percentage
retention.round(3) * 100

# build a heatmap for retension rates of customers
plt.figure(figsize=(15, 7))
plt.title('Retention rates of customers')
sns.heatmap(data=retention, annot=True, fmt= '.0%', vmin= 0.0, vmax= 0.5, cmap='PuBu_r')
plt.show()

# average quantity for each cohort
grouping = df_heat.groupby(['CohortMonth', 'CohortIndex'])
cohort_data = grouping['Quantity'].mean()
cohort_data = cohort_data.reset_index()
average_quantity = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='Quantity')
average_quantity.round(1)
average_quantity.index = average_quantity.index.date

# build the heat map for average quantity
plt.figure(figsize=(15, 7))
plt.title('Average quantity for each cohort')
sns.heatmap(data=average_quantity, annot= True, vmin= 0.0, vmax= 20, cmap='BuGn_r')
plt.show()

# create a new variable by multiplying the price per unit with Quantity
df['TotalPrice'] = df['Quantity'] * df['Price']
df.head()

# RFM Metrics
# + R : Recency shows how recent the customer has purchased. So that is the duration between today and the last date that the customer has purchased. Today will be the last date within the dataset.
# + F : Frequency shows how frequent the customer purchases. So the number of unique Invoices will be the number of times that he/she purchased.
# + M : Monetary shows the total amount of money that the customer has spent.

# check min and max invoice date
print('Min Invoice Date:', df.InvoiceDate.dt.date.min(), '\nMax Invoice Date:', df.InvoiceDate.dt.date.max())

# 2 days will be added to "today's date" to eliminate the local time differences between stores and to make sure that Recency is always above 0
today_date = df['InvoiceDate'].max() + dt.timedelta(days=2)
today_date

rfm = df.groupby('Customer ID').agg({'InvoiceDate': lambda date: (today_date - date.max()).days, 'Invoice': lambda inv: inv.nunique(), 'TotalPrice': lambda price: price.sum()})
rfm.columns = ['Recency', 'Frequency', 'Monetary']
rfm.head()

# we will rate "Recency" customer who have been active more recently better than the less recent customer, because each company wants its customers to be recent
# we will rate "Frequency" and "Monetary Value" higher label because we want Customer to spend more money and visit more often(that is different order than recency).
# building RFM segments
# range(start, stop, step)
rfm_seg = rfm.copy()

r_labels = range(4, 0, -1)
f_labels = range(1, 5)
m_labels = range(1, 5)
r_quartiles = pd.qcut(rfm_seg['Recency'], q=4, labels= r_labels)
# f_quartiles = pd.qcut(rfm_seg['Frequency'], q=4, labels= f_labels)
m_quartiles = pd.qcut(rfm_seg['Monetary'], q=4, labels= m_labels)
# rfm_seg = rfm_seg.assign(R= r_quartiles, F= f_quartiles, M= m_quartiles)

# frequency is duplicated, solution : https://stackoverflow.com/questions/36880490/why-use-pandas-qcut-return-valueerror-bin-edges-must-be-unique/
def pct_rank_qcut(series, n):
    edges = pd.Series([float(i) / n for i in range(n + 1)])
    f = lambda x: (edges >= x).values.argmax()
    return series.rank(pct=1).apply(f)
f_quartiles = rfm_seg['Frequency'].astype(float)
f_quartiles = pct_rank_qcut(rfm_seg['Frequency'], 4)
rfm_seg = rfm_seg.assign(R= r_quartiles, F= f_quartiles, M= m_quartiles)

# build RFM segments and RFM score
def add_rfm_seg(x):
  return str(int(x['R'])) + str(int(x['F'])) + str(int(x['M']))
rfm_seg['RFM_Segment'] = rfm_seg.apply(add_rfm_seg, axis=1)
rfm_seg['RFM_Score'] = rfm_seg[['R', 'F', 'M']].sum(axis=1)
rfm_seg.head()
# print(type(str(rfm_seg['R'])))

# largest RFM segments is always the best practice to investigate the size of the segments before you use them for targeting or other business application.
rfm_seg.groupby(['RFM_Segment']).size().sort_values(ascending=False)[:5]

# select bottom RFM segment '111' and view top 5 rows
rfm_seg[rfm_seg['RFM_Segment']=='111'].head()

# summary metrics per RFM score
# 'count' column is number of customers
rfm_seg.groupby('RFM_Score').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': ['mean', 'count']}).round(1)

# use RFM score to group customers into Diamond, Platinum, Gold, Silver, Bronze and Iron segments
def segments(df):
  if (df['RFM_Score'] > 11):
    return 'Diamond'
  elif (df['RFM_Score'] > 10) and (df['RFM_Score'] <= 11):
    return 'Platinum'
  elif (df['RFM_Score'] > 8) and (df['RFM_Score'] <= 10):
    return 'Gold'
  elif (df['RFM_Score'] > 6) and (df['RFM_Score'] <= 8):
    return 'Silver'
  elif (df['RFM_Score'] > 4) and (df['RFM_Score'] <= 6):
    return 'Bronze'
  else:
    return 'Iron'

# 'count' column is number of customers
rfm_seg['General_Segment'] = rfm_seg.apply(segments, axis=1)
rfm_seg.groupby('General_Segment').agg({'Recency': 'mean', 'Frequency': 'mean', 'Monetary': ['mean', 'count']}).round(1)

# make a copy array for MiniBatchKMeans
rfm_mbk = rfm.copy()
rfm_mbk.head()

# check rfm variables with no negative values and no null values
check_df(rfm)

# instead of using a pre-defined number of clusters, the customers will be segmented according to their purchasing behaviours.
# the logic behind kmeans is to cluster every observation with regards to their similarities using Sum of Squared Distances (SSD).

# simple segmentation without algorithm
rfm_rfm = rfm[['Recency', 'Frequency', 'Monetary']]
print(rfm_rfm.describe())

# standard deviation(std) is the square root of the variance, variance is the average of all data points within a group.
# mean and variance are not equal
f, ax = plt.subplots(figsize=(14, 13))
plt.subplot(3, 1, 1)
sns.distplot(rfm['Recency'], label= 'Recency')
plt.subplot(3, 1, 2)
sns.distplot(rfm['Frequency'], label= 'Frequency')
plt.subplot(3, 1, 3)
sns.distplot(rfm['Monetary'], label= 'Monetary')
plt.style.use('fivethirtyeight')
plt.tight_layout()
plt.show()

# UnSymmetric distribution of variables (data skewed)
# solution: Logarithmic transformation (positive values only) will manage skewness
# we use these Sequence of structuring pre-processing steps :
# 1. unskew the data - log transformation(only works with positive data)
# 2. standardize to the same average values(normalization forces data to have negative values and log will not work)
# 3. scale to the same standard deviation
# 4. store as a separate array to be used for clustering

# customers having more than Mean + 3 Std (Z-score > 3). will be dropped
# define frequency threshold value and drop customers who exceed the threshold
freq_stats = rfm_seg['Frequency'].describe()
freq_threshold = freq_stats['mean'] + 3 * freq_stats['std']
indx = rfm_seg.loc[rfm_seg['Frequency'] > freq_threshold].index
rfm_seg.drop(index = indx, inplace= True)

# define Monetary value threshold value and drop customers who exceed the threshold
m_stats = rfm_seg['Monetary'].describe()
m_threshold = m_stats['mean'] + 3 * m_stats['std']
indx = rfm_seg.loc[rfm_seg['Monetary'] > m_threshold].index
rfm_seg.drop(index = indx, inplace= True)

f, ax = plt.subplots(figsize=(14, 13))
plt.subplot(3, 1, 1)
sns.distplot(rfm_seg['Recency'], label= 'Recency')
plt.subplot(3, 1, 2)
sns.distplot(rfm_seg['Frequency'], label= 'Frequency')
plt.subplot(3, 1, 3)
sns.distplot(rfm_seg['Monetary'], label= 'Monetary')
plt.style.use('fivethirtyeight')
plt.tight_layout()
plt.show()

# unskew the data - log transformation
rfm_log = rfm[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis=1).round(3)
# rfm_log = np.log(rfm_rfm)
# plot the distribution of RFM values
f, ax = plt.subplots(figsize=(14, 13))
plt.subplot(3, 1, 1)
sns.distplot(rfm_log['Recency'], label= 'Recency')
plt.subplot(3, 1, 2)
sns.distplot(rfm_log['Frequency'], label= 'Frequency')
# type of some values monetary is category
plt.subplot(3, 1, 3)
# print(rfm_log['Monetary'].astype('category'))
# rfm_log[['Monetary']] = rfm_log[['Monetary']].apply(lambda col:pd.Categorical(col).codes)
# print(rfm_log['Monetary'] / 100)
sns.distplot(rfm_log['Monetary'] , label= 'Monetary')
plt.style.use('fivethirtyeight')
plt.tight_layout()
plt.show()

# method above have negative value so we don't use it for segmentation
# and just use it to compare
scaler_rfm = StandardScaler()
scaler_rfm.fit(rfm_log)
# store it separately for clustering
rfm_normalized = scaler_rfm.transform(rfm_log)
rfm_normalized_mbk = rfm_normalized.copy()
print(rfm_normalized)

# Monetary values range from 0 to 349164 whereas Recency and Frequency values range form 1 to a 3 digit number. By using MinMaxScaler, all the values are ranging from 0 to 1 (its default parameter).
sc = MinMaxScaler((0, 1))
df = sc.fit_transform(rfm_log)

# copy df for mini batch kmeans
df_mbk = df.copy()
df[0:5]

# kmeans algorithm
# a random number of cluster is being given as a parameter for testing
kmeans = KMeans(n_clusters=4)
# kmeans = MiniBatchKMeans(n_clusters=4)
k_fit = kmeans.fit(df)

# centroids for the 4 clusters
k_fit.cluster_centers_

# elbow method will be used to determine the optimum number of clusters for rfm dataframe
kmeans = KMeans()
# kmeans = MiniBatchKMeans()
ssd = []
K = range(1, 30)

for k in K:
  kmeans = KMeans(n_clusters=k).fit(df)
  # kmeans = MiniBatchKMeans(n_clusters=k).fit(df)
  ssd.append(kmeans.inertia_)

plt.plot(K, ssd, 'bx-')
plt.xlabel('Distance Residual Sums Per Different k Values')
plt.title('Elbow Method for Optimum Number of Clusters')
plt.show()

# make clear the number of clusters
kmeans = KMeans()
# kmeans = MiniBatchKMeans()
visu = KElbowVisualizer(kmeans, k=(2, 20))
visu.fit(df)
visu.show()

# make sure the number of clusters by checking with Silhouette Method
sil_avg=[]
for s in range(2, 20):
  cluster_sil = KMeans(n_clusters = s).fit_predict(df)
  silhouette_avg = silhouette_score(df, cluster_sil)
  sil_avg.append([s, silhouette_avg])

# create new array
sil_avg = np.array(sil_avg)
plt.plot(sil_avg[:, 0], sil_avg[:, 1], linestyle='dashed')
plt.xlabel('Number of Clusters')
plt.ylabel('Average Silhouette Score')
plt.legend(['avg_sil'])
plt.show()

# average Silhouette Score from 4 to 7 clusters
fig, ax = plt.subplots(2, 2, figsize=(20,17))
axli = ax.flatten()
j = 0
for s in range(4, 8):
  cluster_silhouette = KMeans(n_clusters = s)
  visualizer = SilhouetteVisualizer(cluster_silhouette, colors='yellowbrick', ax= axli[j])
  visualizer.fit(df)
  visualizer.finalize()
  j+=1

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # according to Elbow Method, the optimum number of cluster is 6.
# # kmeans starts with allocating cluster centers randomly while kmeans++ starts with allocation one cluster center randomly and then searches for other centers given the first one.
# # + 'k-means++' parameter selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.
# # + 'max_iter' parameter for the maximum number of iterations of the k-means algorithm for a single run. It is given in order to avoid an endless loop.
# %time kmeans = KMeans(n_clusters=6, init= 'k-means++', n_init=10, random_state=0, max_iter=1000, verbose=0)
# # calculate running time of kmeans
# start_time = time.time()
# kmeans.fit(df)
# end_time = time.time() - start_time
# # print(end_time)
# # kmeans.fit_predict(df)
# clusters = kmeans.labels_
# print('\nTotal cell runtime: ', )

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # optimized kmeans with MiniBatchKMeans algorithm
# # MiniBatchKMeans algorithm on large datasets(n >= 10k observations)
# # however, to get just one interation of clusterization we can use default KMeans algorithm to keep results stable. We will not observe such a dramatic improvement using MiniBatchKMeans in comparison to default KMeans algorithm one just one iteration as we have seen that on n > 1 iterations.
# # it's designed to be used to find the centers approximately, not to label all points, trading speed for accuracy while k-means does not benefit much from "big data"
# %time mbk = MiniBatchKMeans(n_clusters=6, init= 'k-means++', n_init=10, random_state=0, max_no_improvement=10, batch_size=100, verbose=0)
# # calculate running time of MiniBatchKMeans
# start_time_mbk = time.time()
# # NOTE : reduce memory usage for big dataset, let you update the model with incremental data when the whole dataset cannot be loaded into the memory
# # partial_fit is for online clustering were fit is for offline
# # partial_fit update k means estimate on a single mini-batch
# mbk.partial_fit(df_mbk)
# # mbk.fit(df_mbk)
# end_time_mbk = time.time() - start_time_mbk
# # print(end_time_mbk)
# clusters_mbk = mbk.labels_
# print('\nTotal cell runtime: ', )

# segment labels for the first 30 customers
print(clusters[0:50])
print(clusters_mbk[0:50])

# customer id and the segment numbers with kmeans
pd.DataFrame({'Customer ID': rfm.index, 'Clusters': clusters})

# customer id and the segment numbers with MiniBatchKMeans
pd.DataFrame({'Customer ID': rfm_mbk.index, 'Clusters': clusters_mbk})

# options 1 with kmeans
# add cluster numbers as a variable to rfm dataframe
# add 1 to cluster segment number so that it starts with 1 instead of 0
rfm['Cluster'] = clusters
rfm['Cluster'] = rfm['Cluster'] + 1
rfm.groupby('Cluster').agg({'Cluster':'count'})

# options 2 with MiniBatchKMeans
# add cluster numbers as a variable to rfm dataframe
# add 1 to cluster segment number so that it starts with 1 instead of 0
rfm_mbk['Cluster'] = clusters_mbk
rfm_mbk['Cluster'] = rfm_mbk['Cluster'] + 1
rfm_mbk.groupby('Cluster').agg({'Cluster':'count'})

# options 1 with kmeans
rfm.head(15)

# options 2 with MiniBatchKMeans
rfm_mbk.head(15)

# calculate average RFM values and size for each cluster
rfm_rfms = rfm_rfm.copy()
rfm_rfm_kmeans = rfm_rfm.assign(Clusters= clusters + 1)
rfm_rfm_mbk = rfm_rfms.assign(Clusters= clusters_mbk + 1)

print(rfm_rfm_kmeans.groupby('Clusters').agg({'Recency': 'mean','Frequency': 'mean', 'Monetary': ['mean', 'count']}).round(0))
print(rfm_rfm_mbk.groupby('Clusters').agg({'Recency': 'mean','Frequency': 'mean', 'Monetary': ['mean', 'count']}).round(0))

# snake plots to understand and compare segments
rfm_normalized = pd.DataFrame(rfm_normalized, index=rfm_rfm.index, columns=rfm_rfm.columns)
rfm_normalized['Clusters'] = clusters + 1
rfm_normalized['General_Segment'] = rfm_seg['General_Segment']
rfm_normalized.reset_index(inplace= True)

rfm_melt = pd.melt(rfm_normalized, id_vars=['Customer ID','General_Segment','Clusters'],value_vars=['Recency', 'Frequency', 'Monetary'], var_name='Metric',value_name='Value')
rfm_melt.head()

# snake plots for mini batch kmeans
rfm_normalized_mbk = pd.DataFrame(rfm_normalized_mbk, index=rfm_rfms.index, columns=rfm_rfms.columns)
rfm_normalized_mbk['Clusters'] = clusters_mbk + 1
rfm_normalized_mbk['General_Segment'] = rfm_seg['General_Segment']
rfm_normalized_mbk.reset_index(inplace= True)

rfm_melt_mbk = pd.melt(rfm_normalized_mbk, id_vars=['Customer ID', 'General_Segment', 'Clusters'], value_vars=['Recency', 'Frequency', 'Monetary'], var_name= 'Metric', value_name='Value')
rfm_melt_mbk.head()

# snake plot
f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))
sns.lineplot(x= 'Metric', y= 'Value', hue= 'General_Segment', data= rfm_melt, ax=ax1)
sns.lineplot(x = 'Metric', y = 'Value', hue = 'Clusters', data = rfm_melt, ax=ax2)
sns.lineplot(x = 'Metric', y = 'Value', hue = 'Clusters', data = rfm_melt_mbk, ax=ax3)
plt.suptitle('Snake Plot of RFM  &&  KMeans  &&  MiniBatchKMeans', fontsize=20)
plt.show()

# useful technique to identify relative importance of each segment's attribute
# calculate average values of each cluster and average values of population
# calculate importance score by dividing them and subtracting 1 (ensures 0 is returned when cluster average equals population average)

# scaled data to standardize with 3 attributes and show in 
# scaler = StandardScaler()
# scaler_mbk = StandardScaler()

kmeans_3 = rfm[['Recency',	'Frequency', 'Monetary']]
mbk_3 = rfm_mbk[['Recency',	'Frequency', 'Monetary']]

kmeans_3['Recency'] = df[:,0]
kmeans_3['Frequency'] = df[:,1]
kmeans_3['Monetary'] = df[:,2]

mbk_3['Recency'] = df_mbk[:,0]
mbk_3['Frequency'] = df_mbk[:,1]
mbk_3['Monetary'] = df_mbk[:,2]

# scaler.fit(kmeans_3)
# scaler_mbk.fit(mbk_3)

# kmeans_scaled = scaler.transform(kmeans_3)
# mbk_scaled = scaler_mbk.transform(mbk_3)

# print(kmeans_scaled)
# print(mbk_scaled)

# scaled cluster center too
# kmeans.cluster_centers_ = scaler.fit_transform(kmeans.cluster_centers_)
# mbk.cluster_centers_ = scaler_mbk.fit_transform(mbk.cluster_centers_)

# 3d version of kmeans
fig_3d_kmeans = px.scatter_3d(
    kmeans_3,
    x= 'Recency',
    y= 'Frequency', 
    z= 'Monetary', 
    color= (clusters + 1).astype(str),
    opacity= 0.7,
    height= 650,
    width= 700,
    title= 'Clusters Obtained using KMeans<br><sup>Training time: %.3fs & Inertia: %f</sup>' % (end_time, kmeans.inertia_),
    color_discrete_sequence = px.colors.qualitative.Set2
)
fig_3d_kmeans.show()

# 3d version of Mini Batch Kmeans
fig_3d_mbk = px.scatter_3d(
    mbk_3,
    x= 'Recency', 
    y= 'Frequency', 
    z= 'Monetary', 
    color= (clusters_mbk + 1).astype(str),
    opacity= 0.7,
    height= 650,
    width= 700,
    title= 'Clusters Obtained using Mini Batch Kmeans<br><sup>Training time: %.3fs & Inertia: %f</sup>' % (end_time_mbk, mbk.inertia_),
    color_discrete_sequence = px.colors.qualitative.Set2
)
fig_3d_mbk.show()

# create 3D chart compare 2 options
# first for kmeans algorithm
fig = plt.figure(figsize=(10, 9))
ax = fig.add_subplot(projection='3d')

ax.scatter(df[:,0], df[:,1], df[:,2], c= clusters, cmap= 'viridis', s= 70, alpha= 0.5)
ax.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], kmeans.cluster_centers_[:,2], s = 350, c = 'r', marker='x', edgecolor='k', label = 'Centroid')
  
plt.title('Clusters Obtained using KMeans\nTraining time: %.3fs & Inertia: %f' % (end_time, kmeans.inertia_),fontsize = 20)
ax.set_xlabel("Recency")
ax.set_ylabel("Frequency")
ax.set_zlabel("Monetory")
plt.autoscale(enable=True, axis='x', tight=True)
kmeansImage = plt.savefig('Kmeans.png', bbox_inches='tight')
plt.show()
plt.close(fig)

# create 3D chart compare 2 options
# second for Mini Batch Kmeans
fig_mbk = plt.figure(figsize=(10, 9))
ax_mbk = fig_mbk.add_subplot(projection='3d')

ax_mbk.scatter(df[:,0], df[:,1], df[:,2], c= clusters_mbk, cmap= 'viridis', s= 70, alpha= 0.5)
ax_mbk.scatter(mbk.cluster_centers_[:,0], mbk.cluster_centers_[:,1], mbk.cluster_centers_[:,2], s = 350, c = 'r', marker='x', edgecolor='k', label = 'Centroid')

plt.title('Clusters Obtained using Mini Batch KMeans\nTraining time: %.3fs & Inertia: %f' % (end_time_mbk, mbk.inertia_), fontsize = 20)
ax_mbk.set_xlabel("Recency")
ax_mbk.set_ylabel("Frequency")
ax_mbk.set_zlabel("Monetory")
plt.autoscale(enable=True, axis='x', tight=True)
mbkImage = plt.savefig('MiniBatchKmeans.png', bbox_inches='tight')
plt.show()
plt.close(fig_mbk)

# show and compare final result image
# inertia is the sum of squared distances of samples to their closest cluster center
imageKmeans = widgets.Image(value=open('Kmeans.png', 'rb').read())
imageMiniBatchKmeans = widgets.Image(value=open('MiniBatchKmeans.png', 'rb').read())
hbox = HBox([imageKmeans, imageMiniBatchKmeans])
display(hbox)

# distribution of customers
df_dis = pd.DataFrame()
df_dis['Distribution Of The KMeans Clusters']= rfm["Cluster"]
df_dis['Distribution Of The Mini Batch KMeans Clusters']= rfm_mbk["Cluster"]


fig, ax =plt.subplots(1,2, figsize=(20,7))
sns.countplot(df_dis['Distribution Of The KMeans Clusters'], ax=ax[0])
sns.countplot(df_dis['Distribution Of The Mini Batch KMeans Clusters'], ax=ax[1])
fig.show()

# recency each clusters
df_re = pd.DataFrame()
df_re['KMeans Clusters']= rfm["Cluster"]
df_re['Mini Batch KMeans Clusters']= rfm_mbk["Cluster"]


fig, (ax1, ax2)=plt.subplots(1,2, figsize=(20,7))
sns.swarmplot(x=df_re['KMeans Clusters'], y=rfm['Recency'], color= "#CBEDDD", alpha=0.5, ax=ax1)
sns.boxenplot(x=df_re['KMeans Clusters'], y=rfm['Recency'], ax=ax1)

sns.swarmplot(x=df_re['Mini Batch KMeans Clusters'], y=rfm_mbk['Recency'], color= "#CBEDDD", alpha=0.5, ax=ax2)
sns.boxenplot(x=df_re['Mini Batch KMeans Clusters'], y=rfm_mbk['Recency'], ax=ax2)
fig.show()

# frequency each clusters
df_fre = pd.DataFrame()
df_fre['KMeans Clusters']= rfm["Cluster"]
df_fre['Mini Batch KMeans Clusters']= rfm_mbk["Cluster"]


fig, (ax1, ax2)=plt.subplots(1,2, figsize=(20,7))
sns.swarmplot(x=df_fre['KMeans Clusters'], y=rfm['Frequency'], color= "#CBEDDD", alpha=0.5, ax=ax1)
sns.boxenplot(x=df_fre['KMeans Clusters'], y=rfm['Frequency'], ax=ax1)

sns.swarmplot(x=df_fre['Mini Batch KMeans Clusters'], y=rfm_mbk['Frequency'], color= "#CBEDDD", alpha=0.5, ax=ax2)
sns.boxenplot(x=df_fre['Mini Batch KMeans Clusters'], y=rfm_mbk['Frequency'], ax=ax2)
fig.show()

# monetary each clusters
df_mon = pd.DataFrame()
df_mon['KMeans Clusters']= rfm["Cluster"]
df_mon['Mini Batch KMeans Clusters']= rfm_mbk["Cluster"]


fig, (ax1, ax2)=plt.subplots(1,2, figsize=(20,7))
sns.swarmplot(x=df_mon['KMeans Clusters'], y=rfm['Monetary'], color= "#CBEDDD", alpha=0.5, ax=ax1)
sns.boxenplot(x=df_mon['KMeans Clusters'], y=rfm['Monetary'], ax=ax1)

sns.swarmplot(x=df_mon['Mini Batch KMeans Clusters'], y=rfm_mbk['Monetary'], color= "#CBEDDD", alpha=0.5, ax=ax2)
sns.boxenplot(x=df_mon['Mini Batch KMeans Clusters'], y=rfm_mbk['Monetary'], ax=ax2)
fig.show()

# sample data for training learning curve kmeans
X, y = mnist_data()
X, y = shuffle_arrays_unison(arrays=[X, y], random_seed=123)
X_train, X_test = X[:4000], X[4000:]
y_train, y_test = y[:4000], y[4000:]
plot_learning_curves(X_train, y_train, X_test, y_test, kmeans, scoring='accuracy', style='fast')
kmeansImage = plt.savefig('KmeansLearningCurve.png', bbox_inches='tight')
plt.show()
plt.close()

# sample data for training learning curve mini batch kmeans
X, y = mnist_data()
X, y = shuffle_arrays_unison(arrays=[X, y], random_seed=123)
X_train, X_test = X[:4000], X[4000:]
y_train, y_test = y[:4000], y[4000:]
plot_learning_curves(X_train, y_train, X_test, y_test, mbk, scoring='accuracy', style='fast')
mbkImage = plt.savefig('MiniBatchKmeansLearningCurve.png', bbox_inches='tight')
plt.show()
plt.close()

# show and compare final result image of learning curve
imageKmeansLearningCurve = widgets.Image(value=open('KmeansLearningCurve.png', 'rb').read())
imageMiniBatchKmeansLearningCurve = widgets.Image(value=open('MiniBatchKmeansLearningCurve.png', 'rb').read())
hbox_curve = HBox([imageKmeansLearningCurve, imageMiniBatchKmeansLearningCurve])
display(hbox_curve)

# convert demo to pdf
!jupyter nbconvert --to html /content/MainProject.ipynb